{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYEs_I3vuodw"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-community\n",
        "!pip install langchain-chroma\n",
        "!pip install pypdf\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import gradio as gr\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI Key\")\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "# Initialize Chat History\n",
        "chat_history = []\n",
        "\n",
        "def create_db_from_pdf(pdf_url: str) -> Chroma:\n",
        "    \"\"\"\n",
        "    Load and process the PDF document into a Chroma vector database.\n",
        "\n",
        "    Args:\n",
        "        pdf_url (str): URL to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        Chroma: The Chroma vector database created from the document.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the PDF document\n",
        "        loader = PyPDFLoader(pdf_url)\n",
        "        data = loader.load()\n",
        "\n",
        "        # Split the document into manageable chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "        chunked_docs = text_splitter.split_documents(data)\n",
        "\n",
        "        # Generate embeddings using OpenAI's model\n",
        "        embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "        db = Chroma.from_documents(\n",
        "            documents=chunked_docs,\n",
        "            embedding=embeddings,\n",
        "            collection_name=\"nestle_policy\",\n",
        "            collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "        return db\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to load and process the PDF: {e}\")\n",
        "\n",
        "def create_chain(db: Chroma, llm: ChatOpenAI) -> Runnable:\n",
        "    \"\"\"\n",
        "    Create the RAG (Retrieval-Augmented Generation) chain using the provided vector database and LLM.\n",
        "\n",
        "    Args:\n",
        "        db (Chroma): The Chroma vector database.\n",
        "        llm (ChatOpenAI): The language model to use for generating responses.\n",
        "\n",
        "    Returns:\n",
        "        Runnable: The RAG chain combining history-aware retrieval and question-answering.\n",
        "    \"\"\"\n",
        "    # Create the similarity-based retriever\n",
        "    similarity_retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.6})\n",
        "\n",
        "    # Define prompts for rephrasing and question-answering\n",
        "    rephrase_system_prompt = \"\"\"\n",
        "    Given a chat history and the latest user question, formulate a standalone question\n",
        "    which can be understood without the chat history. Do NOT answer the question,\n",
        "    just reformulate it if needed and otherwise return it as is.\n",
        "    \"\"\"\n",
        "    qa_system_prompt = \"\"\"\n",
        "    You are a helpful assistant that can answer questions about Human Resource Policies\n",
        "    based on the document provided: The Nestle Human Resources Policy.\n",
        "    You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    Only use the factual information from the document to answer the question.\n",
        "    If you don't know the answer, just say \"I don't know\".\n",
        "    Keep the answer concise and within 5 lines unless the user asks for more information.\n",
        "    Context:\n",
        "    {context}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create chat templates for rephrasing and question-answering\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", rephrase_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    qa_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    # Create chains for rephrasing and question answering\n",
        "    history_aware_retriever = create_history_aware_retriever(llm, similarity_retriever, rephrase_prompt)\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    # Combine retrieval and question-answering into a single chain\n",
        "    qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "    return qa_rag_chain\n",
        "\n",
        "def generate_response(chain: Runnable, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response from the RAG chain based on the given user question.\n",
        "\n",
        "    Args:\n",
        "        chain (Runnable): The RAG chain for generating responses.\n",
        "        question (str): The user question to answer.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer generated by the chatbot.\n",
        "    \"\"\"\n",
        "    # Invoke the RAG chain with the input question and chat history\n",
        "    response = chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "\n",
        "    # Append messages to chat history for context\n",
        "    chat_history.append(HumanMessage(content=question))\n",
        "    chat_history.append(AIMessage(content=response[\"answer\"]))\n",
        "\n",
        "    return response[\"answer\"]\n",
        "\n",
        "def ui_chatbot(input, history):\n",
        "    \"\"\"\n",
        "    Gradio UI callback function for the chatbot.\n",
        "\n",
        "    Args:\n",
        "        input (str): User's input message.\n",
        "        history (list): History of chat messages.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Updated input and chat history.\n",
        "    \"\"\"\n",
        "    # Generate response and update history\n",
        "    bot_message = generate_response(qa_chain, input)\n",
        "    history.append((input, bot_message))\n",
        "    return \"\", history\n",
        "\n",
        "# Create the Chroma vector database and RAG chain\n",
        "chroma_db = create_db_from_pdf(\"https://www.nestle.com/sites/default/files/asset-library/documents/jobs/humanresourcespolicy.pdf\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "qa_chain = create_chain(chroma_db, llm)\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    # Title and Description\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # üè¢ Nestl√© HR Policy Chatbot\n",
        "        **Ask questions about Nestl√©'s HR policies and get responses directly from the document.**\n",
        "\n",
        "        This chatbot answer questions based on the **Nestl√© Human Resources Policy**.\n",
        "        \"\"\"\n",
        "    )\n",
        "    # Gradio components\n",
        "    chatbot = gr.Chatbot(height=300)  # Adjust height for better visualization\n",
        "    msg = gr.Textbox(label=\"Ask a question about HR Policy\")\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    # Link Gradio components to functions\n",
        "    btn.click(ui_chatbot, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(ui_chatbot, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "\n",
        "# Launch Gradio demo\n",
        "gr.close_all()  # Close any previously running Gradio demos\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "IAnmvEmbN22i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}